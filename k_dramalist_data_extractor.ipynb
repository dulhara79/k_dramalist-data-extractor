{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install"
      ],
      "metadata": {
        "id": "5fPPaQGdiq23"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "qTR7HYAtiYUA"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "-_nmczjUiWyL"
      },
      "outputs": [],
      "source": [
        "async def get_drama_links(page_num):\n",
        "    url = f\"https://mydramalist.com/search?adv=titles&ty=68&co=3&st=3&page={page_num}\"\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        context = await browser.new_context(user_agent=\"Mozilla/5.0\")\n",
        "        page = await context.new_page()\n",
        "        await page.goto(url, timeout=60000, wait_until='domcontentloaded')\n",
        "        await page.wait_for_selector(\"h6.title a\", timeout=10000)\n",
        "        content = await page.content()\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        await browser.close()\n",
        "\n",
        "        # Extract drama links\n",
        "        links = []\n",
        "        for title in soup.select(\"h6.title a[href^='/']\"):\n",
        "            href = title.get(\"href\")\n",
        "            if not href.startswith('/search'):\n",
        "                links.append(\"https://mydramalist.com\" + href)\n",
        "        return links\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_drama_details(url):\n",
        "    try:\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(headless=True)\n",
        "            context = await browser.new_context(user_agent=\"Mozilla/5.0\")\n",
        "            page = await context.new_page()\n",
        "            await page.goto(url, timeout=60000, wait_until='domcontentloaded')\n",
        "            content = await page.content()\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            await browser.close()\n",
        "\n",
        "            def safe_get_text(element, selector, default=\"\"):\n",
        "                found = element.select_one(selector)\n",
        "                return found.get_text(strip=True) if found else default\n",
        "\n",
        "            # Extract title\n",
        "            title = safe_get_text(soup, \"h1.film-title\")\n",
        "\n",
        "            details_box = soup.select_one(\"div.box-body.light-b\")\n",
        "\n",
        "            # Extract year\n",
        "            year_element = soup.select_one(\"span.release-year\") or soup.select_one(\"div.film-subtitle\")\n",
        "            year_text = year_element.get_text(strip=True) if year_element else \"\"\n",
        "\n",
        "            # Extract the year using regular expression\n",
        "            year_match = re.search(r'(\\d{4})(?!.*\\d)', year_text)  # Finds last 4-digit number\n",
        "            year = year_match.group(1) if year_match else \"\"\n",
        "\n",
        "            # Extract aired date\n",
        "            aired_text = safe_get_text(details_box, \"li.list-item:has(b:contains('Aired'))\")\n",
        "            aired_match = re.search(r'(\\w+\\s+\\d{1,2},\\s+\\d{4})\\s*-\\s*(\\w+\\s+\\d{1,2},\\s+\\d{4})?', aired_text)\n",
        "            start_date = aired_match.group(1) if aired_match else \"\"\n",
        "            end_date = aired_match.group(2) if aired_match and aired_match.group(2) else \"\"\n",
        "\n",
        "            # Extract original network\n",
        "            network = safe_get_text(details_box, \"li.list-item:has(b:contains('Original Network'))\").replace(\"Original Network:\", \"\").strip()\n",
        "\n",
        "            # Extract aired on\n",
        "            aired_on = safe_get_text(details_box, \"li.list-item:has(b:contains('Aired On'))\").replace(\"Aired On:\", \"\").strip()\n",
        "\n",
        "            # Extract number of episodes\n",
        "            episodes = safe_get_text(details_box, \"li.list-item:has(b:contains('Episodes'))\").replace(\"Episodes:\", \"\").strip()\n",
        "\n",
        "            # Extract duration\n",
        "            duration = safe_get_text(details_box, \"li.list-item:has(b:contains('Duration'))\").replace(\"Duration:\", \"\").strip()\n",
        "\n",
        "            # Extract content rating\n",
        "            content_rating = safe_get_text(details_box, \"li.list-item:has(b:contains('Content Rating'))\").replace(\"Content Rating:\", \"\").strip()\n",
        "\n",
        "            # Extract rating\n",
        "            rating = safe_get_text(soup, \"div.hfs > b\")\n",
        "\n",
        "            # Extract rating (score)\n",
        "            score = safe_get_text(details_box, \"li.list-item:has(b:contains('Score'))\").replace(\"Score:\", \"\").strip()\n",
        "\n",
        "            # Extract genres\n",
        "            genres = [g.get_text(strip=True) for g in soup.select(\"li.list-item a[href*='/search?adv=titles&ge=']\")] if soup.select(\"li.list-item a[href*='/search?adv=titles&ge=']\") else []\n",
        "\n",
        "            # Extract tags\n",
        "            tags = [t.get_text(strip=True) for t in soup.select(\"li.show-tags a[href*='/search?adv=titles&th=']\")] if soup.select(\"li.show-tags a[href*='/search?adv=titles&th=']\") else []\n",
        "\n",
        "            # Extract synopsis\n",
        "            synopsis = safe_get_text(soup, \"div.show-synopsis\")\n",
        "\n",
        "            # Extract statistics\n",
        "            ranked = safe_get_text(details_box, \"li.list-item:has(b:contains('Ranked'))\").replace(\"Ranked:\", \"\").strip()\n",
        "            popularity = safe_get_text(details_box, \"li.list-item:has(b:contains('Popularity'))\").replace(\"Popularity:\", \"\").strip()\n",
        "            watchers = safe_get_text(details_box, \"li.list-item:has(b:contains('Watchers'))\").replace(\"Watchers:\", \"\").strip()\n",
        "\n",
        "            # Extract directors\n",
        "            directors = [d.get_text(strip=True) for d in soup.select(\"li.list-item:has(b:contains('Director')) a.text-primary\")] if soup.select(\"li.list-item:has(b:contains('Director')) a.text-primary\") else []\n",
        "\n",
        "            # Extract screenwriters\n",
        "            screenwriters = [w.get_text(strip=True) for w in soup.select(\"li.list-item:has(b:contains('Screenwriter')) a.text-primary\")] if soup.select(\"li.list-item:has(b:contains('Screenwriter')) a.text-primary\") else []\n",
        "\n",
        "            # Extract main actors and their character names from Cast & Crew section\n",
        "            main_actors = []\n",
        "            cast_elements = soup.select(\"ul.credits li.list-item\")[:4] if soup.select(\"ul.credits li.list-item\") else []\n",
        "            for cast in cast_elements:\n",
        "                actor_name = safe_get_text(cast, \"b[itemprop='name']\")\n",
        "                character_element = cast.select_one(\"small a.text-primary\") or cast.select_one(\"small\")\n",
        "                character_name = character_element.get_text(strip=True) if character_element else \"\"\n",
        "                role_type = \"Main Role\" if \"Main Role\" in cast.get_text(strip=True) else \"Supporting Role\"\n",
        "                main_actors.append({\n",
        "                    \"name\": actor_name,\n",
        "                    \"character\": character_name,\n",
        "                    \"role_type\": role_type\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"title\": title,\n",
        "                \"year\": year,\n",
        "                \"start_date\": start_date,\n",
        "                \"end_date\": end_date,\n",
        "                \"original_network\": network,\n",
        "                \"aired_on\": aired_on,\n",
        "                \"number_of_episodes\": episodes,\n",
        "                \"duration\": duration,\n",
        "                \"content_rating\": content_rating,\n",
        "                \"rating\": rating,\n",
        "                \"genres\": genres,\n",
        "                \"tags\": tags,\n",
        "                \"synopsis\": synopsis,\n",
        "                \"statistics\": {\n",
        "                    \"score\": score,\n",
        "                    \"ranked\": ranked,\n",
        "                    \"popularity\": popularity,\n",
        "                    \"watchers\": watchers\n",
        "                },\n",
        "                \"directors\": directors,\n",
        "                \"screenwriters\": screenwriters,\n",
        "                \"main_actors\": main_actors,\n",
        "                \"url\": url\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {url}: {str(e)}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "3roVU3IAVdde"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_drama_details(url):\n",
        "    try:\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(headless=True)\n",
        "            context = await browser.new_context(user_agent=\"Mozilla/5.0\")\n",
        "            page = await context.new_page()\n",
        "            await page.goto(url, timeout=60000, wait_until='domcontentloaded')\n",
        "            content = await page.content()\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            await browser.close()\n",
        "\n",
        "            def safe_get_text(element, selector, default=\"\"):\n",
        "                found = element.select_one(selector)\n",
        "                return found.get_text(strip=True) if found else default\n",
        "\n",
        "            # Extract fields with safe handling\n",
        "            title = safe_get_text(soup, \"h1.film-title\")\n",
        "\n",
        "            # Extract year from film-subtitle if release-year not found\n",
        "            year_element = soup.select_one(\"span.release-year\") or soup.select_one(\"div.film-subtitle\")\n",
        "            year_text = year_element.get_text(strip=True) if year_element else \"\"\n",
        "\n",
        "            # Extract the year using regular expression\n",
        "            year_match = re.search(r'(\\d{4})(?!.*\\d)', year_text)  # Finds last 4-digit number\n",
        "            year = year_match.group(1) if year_match else \"\"\n",
        "\n",
        "            rating = safe_get_text(soup, \"div.hfs > b\")\n",
        "\n",
        "            # Extract genres\n",
        "            genres = [g.get_text(strip=True) for g in soup.select(\"li.list-item a[href*='/search?adv=titles&ge=']\")] if soup.select(\"li.list-item a[href*='/search?adv=titles&ge=']\") else []\n",
        "\n",
        "            # Extract tags\n",
        "            tags = [t.get_text(strip=True) for t in soup.select(\"li.show-tags a[href*='/search?adv=titles&th=']\")] if soup.select(\"li.show-tags a[href*='/search?adv=titles&th=']\") else []\n",
        "\n",
        "            # Extract synopsis\n",
        "            synopsis = safe_get_text(soup, \"div.show-synopsis\")\n",
        "\n",
        "            # Extract statistics\n",
        "            stats_box = soup.select_one(\"div.box-body.light-b\")\n",
        "            score = safe_get_text(stats_box, \"li.list-item:has(b:contains('Score'))\", \"\").replace(\"Score\", \"\").strip()\n",
        "            ranked = safe_get_text(stats_box, \"li.list-item:has(b:contains('Ranked'))\", \"\").replace(\"Ranked\", \"\").strip()\n",
        "            popularity = safe_get_text(stats_box, \"li.list-item:has(b:contains('Popularity'))\", \"\").replace(\"Popularity\", \"\").strip()\n",
        "            watchers = safe_get_text(stats_box, \"li.list-item:has(b:contains('Watchers'))\", \"\").replace(\"Watchers\", \"\").strip()\n",
        "\n",
        "            # Extract directors\n",
        "            directors = [d.get_text(strip=True) for d in soup.select(\"li.list-item:has(b:contains('Director')) a.text-primary\")] if soup.select(\"li.list-item:has(b:contains('Director')) a.text-primary\") else []\n",
        "\n",
        "            # Extract screenwriters\n",
        "            screenwriters = [w.get_text(strip=True) for w in soup.select(\"li.list-item:has(b:contains('Screenwriter')) a.text-primary\")] if soup.select(\"li.list-item:has(b:contains('Screenwriter')) a.text-primary\") else []\n",
        "\n",
        "            # Extract main actors (top 4 listed in cast section)\n",
        "            main_actors = []\n",
        "            actor_elements = soup.select(\"ul.credits li.list-item\")[:4] if soup.select(\"ul.credits li.list-item\") else []\n",
        "            for actor in actor_elements:\n",
        "                actor_name = safe_get_text(actor, \"b[itemprop='name']\")\n",
        "                character_element = actor.select_one(\"small a.text-primary\") or actor.select_one(\"small\")\n",
        "                character_name = character_element.get_text(strip=True) if character_element else \"\"\n",
        "                main_actors.append({\n",
        "                    \"name\": actor_name,\n",
        "                    \"character\": character_name\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"title\": title,\n",
        "                \"year\": year,\n",
        "                \"rating\": rating,\n",
        "                \"genres\": genres,\n",
        "                \"tags\": tags,\n",
        "                \"synopsis\": synopsis,\n",
        "                \"statistics\": {\n",
        "                    \"score\": score,\n",
        "                    \"ranked\": ranked,\n",
        "                    \"popularity\": popularity,\n",
        "                    \"watchers\": watchers\n",
        "                },\n",
        "                \"directors\": directors,\n",
        "                \"screenwriters\": screenwriters,\n",
        "                \"main_actors\": main_actors,\n",
        "                \"url\": url\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch {url}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "40uD4kHpid0j"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    all_data = []\n",
        "\n",
        "    for page_num in range(1, 204):  # Scrape first 1 pages\n",
        "        print(f\"Scraping page {page_num}...\")\n",
        "        url = f\"https://mydramalist.com/search?adv=titles&ty=68,77,83,86&co=3&st=3&so=top&page={page_num}\"\n",
        "        drama_links = await get_drama_links(url)\n",
        "\n",
        "        for link in drama_links:\n",
        "            data = await get_drama_details(link)\n",
        "            if data:  # Only append if data was successfully scraped\n",
        "                all_data.append(data)\n",
        "\n",
        "    # Only save to CSV if we have data\n",
        "    if all_data:\n",
        "        keys = all_data[0].keys()\n",
        "        with open('drama_dataset.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "            dict_writer = csv.DictWriter(f, keys)\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(all_data)\n",
        "        print(\"Data saved to drama_dataset.csv\")\n",
        "    else:\n",
        "        print(\"No data was scraped successfully\")"
      ],
      "metadata": {
        "id": "nMuOEo_ypOnP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Jupyter or Colab\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()  # Patch the running event loop\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNSbf_LGj52G",
        "outputId": "69a1999f-adbb-465b-fcf9-e1f29e2fc3d2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Scraping page 52...\n",
            "Scraping page 53...\n",
            "Scraping page 54...\n",
            "Scraping page 55...\n",
            "Scraping page 56...\n",
            "Scraping page 57...\n",
            "Scraping page 58...\n",
            "Scraping page 59...\n",
            "Scraping page 60...\n",
            "Scraping page 61...\n",
            "Scraping page 62...\n",
            "Scraping page 63...\n",
            "Scraping page 64...\n",
            "Scraping page 65...\n",
            "Scraping page 66...\n",
            "Scraping page 67...\n",
            "Scraping page 68...\n",
            "Scraping page 69...\n",
            "Scraping page 70...\n",
            "Scraping page 71...\n",
            "Scraping page 72...\n",
            "Scraping page 73...\n",
            "Scraping page 74...\n",
            "Scraping page 75...\n",
            "Scraping page 76...\n",
            "Scraping page 77...\n",
            "Scraping page 78...\n",
            "Scraping page 79...\n",
            "Scraping page 80...\n",
            "Scraping page 81...\n",
            "Scraping page 82...\n",
            "Scraping page 83...\n",
            "Scraping page 84...\n",
            "Scraping page 85...\n",
            "Scraping page 86...\n",
            "Scraping page 87...\n",
            "Scraping page 88...\n",
            "Scraping page 89...\n",
            "Scraping page 90...\n",
            "Scraping page 91...\n",
            "Scraping page 92...\n",
            "Scraping page 93...\n",
            "Scraping page 94...\n",
            "Scraping page 95...\n",
            "Scraping page 96...\n",
            "Scraping page 97...\n",
            "Scraping page 98...\n",
            "Scraping page 99...\n",
            "Scraping page 100...\n",
            "Scraping page 101...\n",
            "Scraping page 102...\n",
            "Scraping page 103...\n",
            "Scraping page 104...\n",
            "Scraping page 105...\n",
            "Scraping page 106...\n",
            "Scraping page 107...\n",
            "Scraping page 108...\n",
            "Scraping page 109...\n",
            "Scraping page 110...\n",
            "Scraping page 111...\n",
            "Scraping page 112...\n",
            "Scraping page 113...\n",
            "Scraping page 114...\n",
            "Scraping page 115...\n",
            "Scraping page 116...\n",
            "Scraping page 117...\n",
            "Scraping page 118...\n",
            "Scraping page 119...\n",
            "Scraping page 120...\n",
            "Scraping page 121...\n",
            "Scraping page 122...\n",
            "Scraping page 123...\n",
            "Scraping page 124...\n",
            "Scraping page 125...\n",
            "Scraping page 126...\n",
            "Scraping page 127...\n",
            "Scraping page 128...\n",
            "Scraping page 129...\n",
            "Scraping page 130...\n",
            "Scraping page 131...\n",
            "Scraping page 132...\n",
            "Scraping page 133...\n",
            "Scraping page 134...\n",
            "Scraping page 135...\n",
            "Scraping page 136...\n",
            "Scraping page 137...\n",
            "Scraping page 138...\n",
            "Scraping page 139...\n",
            "Scraping page 140...\n",
            "Scraping page 141...\n",
            "Scraping page 142...\n",
            "Scraping page 143...\n",
            "Scraping page 144...\n",
            "Scraping page 145...\n",
            "Scraping page 146...\n",
            "Scraping page 147...\n",
            "Scraping page 148...\n",
            "Scraping page 149...\n",
            "Scraping page 150...\n",
            "Scraping page 151...\n",
            "Scraping page 152...\n",
            "Scraping page 153...\n",
            "Scraping page 154...\n",
            "Scraping page 155...\n",
            "Scraping page 156...\n",
            "Scraping page 157...\n",
            "Scraping page 158...\n",
            "Scraping page 159...\n",
            "Scraping page 160...\n",
            "Scraping page 161...\n",
            "Scraping page 162...\n",
            "Scraping page 163...\n",
            "Scraping page 164...\n",
            "Scraping page 165...\n",
            "Scraping page 166...\n",
            "Scraping page 167...\n",
            "Scraping page 168...\n",
            "Scraping page 169...\n",
            "Scraping page 170...\n",
            "Scraping page 171...\n",
            "Scraping page 172...\n",
            "Scraping page 173...\n",
            "Scraping page 174...\n",
            "Scraping page 175...\n",
            "Scraping page 176...\n",
            "Scraping page 177...\n",
            "Scraping page 178...\n",
            "Scraping page 179...\n",
            "Scraping page 180...\n",
            "Scraping page 181...\n",
            "Scraping page 182...\n",
            "Scraping page 183...\n",
            "Scraping page 184...\n",
            "Scraping page 185...\n",
            "Scraping page 186...\n",
            "Scraping page 187...\n",
            "Scraping page 188...\n",
            "Scraping page 189...\n",
            "Scraping page 190...\n",
            "Scraping page 191...\n",
            "Scraping page 192...\n",
            "Scraping page 193...\n",
            "Scraping page 194...\n",
            "Scraping page 195...\n",
            "Scraping page 196...\n",
            "Scraping page 197...\n",
            "Scraping page 198...\n",
            "Scraping page 199...\n",
            "Scraping page 200...\n",
            "Scraping page 201...\n",
            "Scraping page 202...\n",
            "Scraping page 203...\n",
            "Data saved to drama_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZNA4u5FsEVK"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}